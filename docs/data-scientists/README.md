# ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆå‘ã‘ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

> **æ³¨æ„**: ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯æ®µéšçš„ã«ä½œæˆä¸­ã§ã™ã€‚å¤šãã®ãƒªãƒ³ã‚¯å…ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒ **(æº–å‚™ä¸­)** çŠ¶æ…‹ã§ã™ã€‚

## ğŸ“‹ æ¦‚è¦

ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ•ãƒ©ã‚°ã‚·ã‚¹ãƒ†ãƒ ã§åé›†ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€A/Bãƒ†ã‚¹ãƒˆã®åŠ¹æœæ¸¬å®šã‚„æ©Ÿèƒ½ã®æ”¹å–„ææ¡ˆã‚’è¡Œã†ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆå‘ã‘ã®æƒ…å ±ã‚’æä¾›ã—ã¾ã™ã€‚

## ğŸ¯ ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã®è²¬å‹™

### ãƒ‡ãƒ¼ã‚¿åˆ†æ
- âœ… A/Bãƒ†ã‚¹ãƒˆçµæœã®çµ±è¨ˆåˆ†æ
- âœ… ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•åˆ†æ
- âœ… æ©Ÿèƒ½åŠ¹æœæ¸¬å®š
- âœ… äºˆæ¸¬åˆ†æãƒ»æ©Ÿæ¢°å­¦ç¿’

### åŠ¹æœæ¸¬å®š
- âœ… KPIè¨­å®šãƒ»è¿½è·¡
- âœ… çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œå®š
- âœ… åŠ¹æœã‚µã‚¤ã‚ºè¨ˆç®—
- âœ… ROIåˆ†æ

### æ´å¯Ÿæä¾›
- âœ… ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ–ãƒ³ãªæ„æ€æ±ºå®šæ”¯æ´
- âœ… ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
- âœ… æ©Ÿèƒ½æ”¹å–„ææ¡ˆ
- âœ… äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### ğŸ’¡ æœ€åˆã«ã‚„ã‚‹ã“ã¨
1. [ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®ç†è§£](#ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹)
2. [åˆ†æç’°å¢ƒã®æ§‹ç¯‰](#åˆ†æç’°å¢ƒ)
3. [A/Bãƒ†ã‚¹ãƒˆåˆ†æã®åŸºæœ¬](#ABãƒ†ã‚¹ãƒˆåˆ†æ)
4. [çµ±è¨ˆçš„æ‰‹æ³•ã®é©ç”¨](#çµ±è¨ˆæ‰‹æ³•)

### ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
```sql
-- ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ•ãƒ©ã‚°é–¢é€£ã®ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«
-- 1. ãƒ•ãƒ©ã‚°è©•ä¾¡ãƒ­ã‚°
CREATE TABLE flag_evaluations (
    id SERIAL PRIMARY KEY,
    flag_key VARCHAR(255) NOT NULL,
    user_id VARCHAR(255) NOT NULL,
    tenant_id VARCHAR(255),
    enabled BOOLEAN NOT NULL,
    variation VARCHAR(100),
    context JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- 2. ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ã‚¤ãƒ™ãƒ³ãƒˆ
CREATE TABLE user_events (
    id SERIAL PRIMARY KEY,
    user_id VARCHAR(255) NOT NULL,
    event_type VARCHAR(100) NOT NULL,
    event_data JSONB,
    session_id VARCHAR(255),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- 3. ãƒ“ã‚¸ãƒã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹
CREATE TABLE business_metrics (
    id SERIAL PRIMARY KEY,
    user_id VARCHAR(255) NOT NULL,
    metric_type VARCHAR(100) NOT NULL,
    metric_value DECIMAL(10,2),
    currency VARCHAR(3),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

## ğŸ“š åˆ†æã‚¬ã‚¤ãƒ‰

### ğŸ“– åŸºæœ¬åˆ†æï¼ˆæ¨å®šæ™‚é–“: 4-5æ™‚é–“ï¼‰
1. [ãƒ‡ãƒ¼ã‚¿æ¢ç´¢åˆ†æ](./exploratory-data-analysis.md)
2. [è¨˜è¿°çµ±è¨ˆ](./descriptive-statistics.md)
3. [ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–](./data-visualization.md)
4. [ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡](./data-quality-assessment.md)

### ğŸ§ª A/Bãƒ†ã‚¹ãƒˆåˆ†æï¼ˆæ¨å®šæ™‚é–“: 6-8æ™‚é–“ï¼‰
1. [A/Bãƒ†ã‚¹ãƒˆè¨­è¨ˆ](./ab-test-design.md)
2. [ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºè¨ˆç®—](./sample-size-calculation.md)
3. [çµ±è¨ˆçš„æ¤œå®š](./statistical-testing.md)
4. [åŠ¹æœã‚µã‚¤ã‚ºè¨ˆç®—](./effect-size-calculation.md)

### ğŸ“Š é«˜åº¦ãªåˆ†æï¼ˆæ¨å®šæ™‚é–“: 8-10æ™‚é–“ï¼‰
1. [å¤šå¤‰é‡è§£æ](./multivariate-analysis.md)
2. [æ©Ÿæ¢°å­¦ç¿’å¿œç”¨](./machine-learning-applications.md)
3. [äºˆæ¸¬ãƒ¢ãƒ‡ãƒªãƒ³ã‚°](./predictive-modeling.md)
4. [å› æœæ¨è«–](./causal-inference.md)

### ğŸ¯ ãƒ“ã‚¸ãƒã‚¹åˆ†æï¼ˆæ¨å®šæ™‚é–“: 4-6æ™‚é–“ï¼‰
1. [ROIåˆ†æ](./roi-analysis.md)
2. [ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³](./user-segmentation.md)
3. [ã‚³ãƒ›ãƒ¼ãƒˆåˆ†æ](./cohort-analysis.md)
4. [LTVåˆ†æ](./ltv-analysis.md)

## ğŸ§ª A/Bãƒ†ã‚¹ãƒˆåˆ†æ

### ğŸ“Š å®Ÿé¨“è¨­è¨ˆ
#### A/Bãƒ†ã‚¹ãƒˆè¨­è¨ˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
```python
import pandas as pd
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns

class ABTestDesigner:
    def __init__(self):
        self.alpha = 0.05  # æœ‰æ„æ°´æº–
        self.power = 0.8   # æ¤œå‡ºåŠ›
        
    def calculate_sample_size(self, baseline_rate, minimum_detectable_effect):
        """
        ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºè¨ˆç®—
        """
        effect_size = minimum_detectable_effect / baseline_rate
        z_alpha = stats.norm.ppf(1 - self.alpha/2)
        z_beta = stats.norm.ppf(self.power)
        
        # æ¯”ç‡ã®å·®ã®æ¤œå®šç”¨ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º
        p1 = baseline_rate
        p2 = baseline_rate * (1 + effect_size)
        p_pooled = (p1 + p2) / 2
        
        sample_size = ((z_alpha * np.sqrt(2 * p_pooled * (1 - p_pooled)) + 
                       z_beta * np.sqrt(p1 * (1 - p1) + p2 * (1 - p2))) / 
                      (p2 - p1)) ** 2
        
        return int(np.ceil(sample_size))
    
    def design_experiment(self, metric_name, baseline_rate, 
                         minimum_detectable_effect, duration_days):
        """
        å®Ÿé¨“è¨­è¨ˆ
        """
        sample_size = self.calculate_sample_size(baseline_rate, minimum_detectable_effect)
        
        design = {
            'metric': metric_name,
            'baseline_rate': baseline_rate,
            'minimum_detectable_effect': minimum_detectable_effect,
            'sample_size_per_group': sample_size,
            'total_sample_size': sample_size * 2,
            'duration_days': duration_days,
            'daily_sample_requirement': int(sample_size * 2 / duration_days),
            'alpha': self.alpha,
            'power': self.power
        }
        
        return design
```

### ğŸ“ˆ çµæœåˆ†æ
#### çµ±è¨ˆçš„æ¤œå®šã¨åŠ¹æœæ¸¬å®š
```python
class ABTestAnalyzer:
    def __init__(self):
        self.alpha = 0.05
        
    def analyze_proportions(self, control_successes, control_total, 
                           treatment_successes, treatment_total):
        """
        æ¯”ç‡ã®æ¯”è¼ƒï¼ˆã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç‡ãªã©ï¼‰
        """
        # åŸºæœ¬çµ±è¨ˆ
        control_rate = control_successes / control_total
        treatment_rate = treatment_successes / treatment_total
        
        # çµ±è¨ˆçš„æ¤œå®š
        z_stat, p_value = stats.proportions_ztest(
            [control_successes, treatment_successes],
            [control_total, treatment_total]
        )
        
        # åŠ¹æœã‚µã‚¤ã‚º
        effect_size = (treatment_rate - control_rate) / control_rate
        
        # ä¿¡é ¼åŒºé–“
        diff = treatment_rate - control_rate
        se_diff = np.sqrt(
            control_rate * (1 - control_rate) / control_total +
            treatment_rate * (1 - treatment_rate) / treatment_total
        )
        ci_lower = diff - 1.96 * se_diff
        ci_upper = diff + 1.96 * se_diff
        
        return {
            'control_rate': control_rate,
            'treatment_rate': treatment_rate,
            'absolute_difference': diff,
            'relative_difference': effect_size,
            'z_statistic': z_stat,
            'p_value': p_value,
            'is_significant': p_value < self.alpha,
            'confidence_interval': (ci_lower, ci_upper)
        }
    
    def analyze_means(self, control_values, treatment_values):
        """
        å¹³å‡å€¤ã®æ¯”è¼ƒï¼ˆå£²ä¸Šã€æ»åœ¨æ™‚é–“ãªã©ï¼‰
        """
        # åŸºæœ¬çµ±è¨ˆ
        control_mean = np.mean(control_values)
        treatment_mean = np.mean(treatment_values)
        control_std = np.std(control_values, ddof=1)
        treatment_std = np.std(treatment_values, ddof=1)
        
        # tæ¤œå®š
        t_stat, p_value = stats.ttest_ind(treatment_values, control_values)
        
        # åŠ¹æœã‚µã‚¤ã‚º (Cohen's d)
        pooled_std = np.sqrt(((len(control_values) - 1) * control_std**2 + 
                            (len(treatment_values) - 1) * treatment_std**2) / 
                           (len(control_values) + len(treatment_values) - 2))
        cohens_d = (treatment_mean - control_mean) / pooled_std
        
        return {
            'control_mean': control_mean,
            'treatment_mean': treatment_mean,
            'difference': treatment_mean - control_mean,
            'relative_difference': (treatment_mean - control_mean) / control_mean,
            't_statistic': t_stat,
            'p_value': p_value,
            'is_significant': p_value < self.alpha,
            'cohens_d': cohens_d
        }
    
    def multiple_testing_correction(self, p_values, method='bonferroni'):
        """
        å¤šé‡æ¤œå®šè£œæ­£
        """
        from statsmodels.stats.multitest import multipletests
        
        rejected, corrected_p_values, alpha_sidak, alpha_bonf = multipletests(
            p_values, alpha=self.alpha, method=method
        )
        
        return {
            'original_p_values': p_values,
            'corrected_p_values': corrected_p_values,
            'rejected': rejected,
            'corrected_alpha': alpha_bonf if method == 'bonferroni' else alpha_sidak
        }
```

## ğŸ“Š ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–

### ğŸ“ˆ A/Bãƒ†ã‚¹ãƒˆçµæœã®å¯è¦–åŒ–
```python
class ABTestVisualizer:
    def __init__(self):
        plt.style.use('seaborn-v0_8')
        
    def plot_conversion_rates(self, results_df):
        """
        ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç‡ã®æ¯”è¼ƒ
        """
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # ãƒãƒ¼ãƒ—ãƒ­ãƒƒãƒˆ
        groups = results_df['group'].unique()
        rates = results_df.groupby('group')['converted'].mean()
        
        ax1.bar(groups, rates, color=['#3498db', '#e74c3c'])
        ax1.set_title('Conversion Rates by Group')
        ax1.set_ylabel('Conversion Rate')
        ax1.set_ylim(0, max(rates) * 1.2)
        
        # ä¿¡é ¼åŒºé–“ä»˜ããƒãƒ¼ãƒ—ãƒ­ãƒƒãƒˆ
        ci_lower = []
        ci_upper = []
        for group in groups:
            group_data = results_df[results_df['group'] == group]
            rate = group_data['converted'].mean()
            n = len(group_data)
            se = np.sqrt(rate * (1 - rate) / n)
            ci_lower.append(rate - 1.96 * se)
            ci_upper.append(rate + 1.96 * se)
        
        ax2.bar(groups, rates, color=['#3498db', '#e74c3c'], alpha=0.7)
        ax2.errorbar(groups, rates, yerr=[np.array(rates) - np.array(ci_lower),
                                         np.array(ci_upper) - np.array(rates)], 
                    fmt='none', color='black', capsize=5)
        ax2.set_title('Conversion Rates with 95% Confidence Intervals')
        ax2.set_ylabel('Conversion Rate')
        
        plt.tight_layout()
        plt.show()
    
    def plot_time_series(self, results_df):
        """
        æ™‚ç³»åˆ—ã§ã®çµæœæ¨ç§»
        """
        fig, ax = plt.subplots(figsize=(12, 6))
        
        # æ—¥åˆ¥ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç‡
        daily_rates = results_df.groupby(['date', 'group'])['converted'].mean().unstack()
        
        daily_rates.plot(ax=ax, marker='o', linewidth=2)
        ax.set_title('Daily Conversion Rates')
        ax.set_xlabel('Date')
        ax.set_ylabel('Conversion Rate')
        ax.legend(title='Group')
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def plot_statistical_power(self, effect_sizes, sample_sizes):
        """
        çµ±è¨ˆçš„æ¤œå‡ºåŠ›ã®å¯è¦–åŒ–
        """
        fig, ax = plt.subplots(figsize=(10, 6))
        
        for effect_size in effect_sizes:
            powers = []
            for n in sample_sizes:
                # æ¤œå‡ºåŠ›è¨ˆç®—
                z_beta = stats.norm.ppf(0.8)  # 80%æ¤œå‡ºåŠ›
                z_alpha = stats.norm.ppf(0.975)  # ä¸¡å´5%
                
                # ç°¡ç•¥åŒ–ã•ã‚ŒãŸæ¤œå‡ºåŠ›è¨ˆç®—
                delta = effect_size
                power = 1 - stats.norm.cdf(z_alpha - delta * np.sqrt(n/2))
                powers.append(power)
            
            ax.plot(sample_sizes, powers, label=f'Effect Size: {effect_size:.1%}')
        
        ax.axhline(y=0.8, color='red', linestyle='--', label='80% Power')
        ax.set_xlabel('Sample Size per Group')
        ax.set_ylabel('Statistical Power')
        ax.set_title('Statistical Power vs Sample Size')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
```

## ğŸ¯ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³

### ğŸ‘¥ ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æ
```python
class UserSegmentAnalyzer:
    def __init__(self):
        pass
    
    def rfm_analysis(self, user_data):
        """
        RFMåˆ†æï¼ˆRecency, Frequency, Monetaryï¼‰
        """
        # æœ€æ–°ã®è³¼å…¥æ—¥ã‹ã‚‰ã®æ—¥æ•°
        user_data['recency'] = (user_data['analysis_date'] - 
                               user_data['last_purchase_date']).dt.days
        
        # è³¼å…¥é »åº¦
        user_data['frequency'] = user_data['purchase_count']
        
        # è³¼å…¥é‡‘é¡
        user_data['monetary'] = user_data['total_spent']
        
        # äº”åˆ†ä½æ•°ã§ã‚¹ã‚³ã‚¢åŒ–
        user_data['r_score'] = pd.qcut(user_data['recency'], 5, 
                                      labels=[5,4,3,2,1]).astype(int)
        user_data['f_score'] = pd.qcut(user_data['frequency'].rank(method='first'), 5, 
                                      labels=[1,2,3,4,5]).astype(int)
        user_data['m_score'] = pd.qcut(user_data['monetary'], 5, 
                                      labels=[1,2,3,4,5]).astype(int)
        
        # RFMã‚¹ã‚³ã‚¢
        user_data['rfm_score'] = (user_data['r_score'].astype(str) + 
                                 user_data['f_score'].astype(str) + 
                                 user_data['m_score'].astype(str))
        
        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†é¡
        def classify_segment(row):
            if row['rfm_score'] in ['555', '554', '544', '545', '454', '455', '445']:
                return 'Champions'
            elif row['rfm_score'] in ['543', '444', '435', '355', '354', '345', '344', '335']:
                return 'Loyal Customers'
            elif row['rfm_score'] in ['553', '551', '552', '541', '542', '533', '532', '531', '452', '451']:
                return 'Potential Loyalists'
            elif row['rfm_score'] in ['512', '511', '422', '421', '412', '411', '311']:
                return 'New Customers'
            elif row['rfm_score'] in ['155', '154', '144', '214', '215', '115', '114']:
                return 'At Risk'
            else:
                return 'Others'
        
        user_data['segment'] = user_data.apply(classify_segment, axis=1)
        
        return user_data
    
    def behavioral_segmentation(self, user_events):
        """
        è¡Œå‹•ãƒ™ãƒ¼ã‚¹ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
        """
        # è¡Œå‹•æŒ‡æ¨™ã®è¨ˆç®—
        user_behavior = user_events.groupby('user_id').agg({
            'session_id': 'nunique',  # ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°
            'page_views': 'sum',      # ãƒšãƒ¼ã‚¸ãƒ“ãƒ¥ãƒ¼æ•°
            'time_spent': 'sum',      # æ»åœ¨æ™‚é–“
            'feature_usage': 'sum'    # æ©Ÿèƒ½ä½¿ç”¨å›æ•°
        }).reset_index()
        
        # æ­£è¦åŒ–
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        scaled_features = scaler.fit_transform(user_behavior.iloc[:, 1:])
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        from sklearn.cluster import KMeans
        kmeans = KMeans(n_clusters=5, random_state=42)
        clusters = kmeans.fit_predict(scaled_features)
        
        user_behavior['cluster'] = clusters
        
        return user_behavior
```

## ğŸ“Š ROIåˆ†æ

### ğŸ’° æŠ•è³‡å¯¾åŠ¹æœåˆ†æ
```python
class ROIAnalyzer:
    def __init__(self):
        pass
    
    def calculate_feature_roi(self, development_cost, maintenance_cost, 
                            revenue_impact, user_impact, time_period_months):
        """
        æ©Ÿèƒ½ã®ROIè¨ˆç®—
        """
        # ç·ã‚³ã‚¹ãƒˆ
        total_cost = development_cost + (maintenance_cost * time_period_months)
        
        # ç·åç›Š
        total_revenue = revenue_impact * time_period_months
        
        # ROIè¨ˆç®—
        roi = (total_revenue - total_cost) / total_cost * 100
        
        # æŠ•è³‡å›åæœŸé–“
        payback_period = total_cost / (revenue_impact if revenue_impact > 0 else 1)
        
        return {
            'total_cost': total_cost,
            'total_revenue': total_revenue,
            'net_profit': total_revenue - total_cost,
            'roi_percentage': roi,
            'payback_period_months': payback_period
        }
    
    def ab_test_roi_analysis(self, control_metrics, treatment_metrics, 
                            traffic_percentage, implementation_cost):
        """
        A/Bãƒ†ã‚¹ãƒˆã®æŠ•è³‡å¯¾åŠ¹æœåˆ†æ
        """
        # åŠ¹æœè¨ˆç®—
        revenue_per_user_control = control_metrics['revenue_per_user']
        revenue_per_user_treatment = treatment_metrics['revenue_per_user']
        
        # å¹´é–“å£²ä¸Šã¸ã®å½±éŸ¿
        annual_users = control_metrics['annual_users']
        annual_revenue_impact = (revenue_per_user_treatment - revenue_per_user_control) * \
                              annual_users * traffic_percentage
        
        # ROIè¨ˆç®—
        roi = (annual_revenue_impact - implementation_cost) / implementation_cost * 100
        
        return {
            'annual_revenue_impact': annual_revenue_impact,
            'implementation_cost': implementation_cost,
            'roi_percentage': roi,
            'break_even_point': implementation_cost / annual_revenue_impact * 365 if annual_revenue_impact > 0 else float('inf')
        }
```

## ğŸ¤– æ©Ÿæ¢°å­¦ç¿’å¿œç”¨

### ğŸ“Š äºˆæ¸¬ãƒ¢ãƒ‡ãƒªãƒ³ã‚°
```python
class FeatureFlagPredictor:
    def __init__(self):
        self.models = {}
    
    def predict_conversion_probability(self, user_features, flag_features):
        """
        ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ•ãƒ©ã‚°æœ‰åŠ¹åŒ–ã«ã‚ˆã‚‹åŠ¹æœäºˆæ¸¬
        """
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import roc_auc_score, classification_report
        
        # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
        X = self.create_features(user_features, flag_features)
        y = user_features['converted']
        
        # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        model.fit(X_train, y_train)
        
        # äºˆæ¸¬
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        
        # è©•ä¾¡
        auc_score = roc_auc_score(y_test, y_pred_proba)
        classification_rep = classification_report(y_test, y_pred)
        
        self.models['conversion_predictor'] = model
        
        return {
            'model': model,
            'auc_score': auc_score,
            'classification_report': classification_rep,
            'feature_importance': dict(zip(X.columns, model.feature_importances_))
        }
    
    def create_features(self, user_features, flag_features):
        """
        ç‰¹å¾´é‡ä½œæˆ
        """
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ç‰¹å¾´é‡
        features = pd.DataFrame({
            'user_age': user_features['age'],
            'user_tenure': user_features['tenure_days'],
            'user_activity_score': user_features['activity_score'],
            'user_segment': pd.Categorical(user_features['segment']).codes,
            'flag_enabled': flag_features['enabled'].astype(int),
            'flag_duration': flag_features['duration_days']
        })
        
        # äº¤äº’ä½œç”¨é …
        features['age_x_flag'] = features['user_age'] * features['flag_enabled']
        features['tenure_x_flag'] = features['user_tenure'] * features['flag_enabled']
        
        return features
    
    def uplift_modeling(self, user_data):
        """
        ã‚¢ãƒƒãƒ—ãƒªãƒ•ãƒˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°
        """
        from sklearn.ensemble import GradientBoostingClassifier
        
        # å‡¦ç†ç¾¤ã¨å¯¾ç…§ç¾¤ã®ãƒ‡ãƒ¼ã‚¿
        treatment_data = user_data[user_data['treatment'] == 1]
        control_data = user_data[user_data['treatment'] == 0]
        
        # å„ç¾¤ã§ã®ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        features = ['age', 'tenure', 'activity_score', 'segment']
        
        # å‡¦ç†ç¾¤ãƒ¢ãƒ‡ãƒ«
        treatment_model = GradientBoostingClassifier(random_state=42)
        treatment_model.fit(treatment_data[features], treatment_data['converted'])
        
        # å¯¾ç…§ç¾¤ãƒ¢ãƒ‡ãƒ«
        control_model = GradientBoostingClassifier(random_state=42)
        control_model.fit(control_data[features], control_data['converted'])
        
        # ã‚¢ãƒƒãƒ—ãƒªãƒ•ãƒˆäºˆæ¸¬
        def predict_uplift(user_features):
            treatment_prob = treatment_model.predict_proba(user_features)[:, 1]
            control_prob = control_model.predict_proba(user_features)[:, 1]
            return treatment_prob - control_prob
        
        return {
            'treatment_model': treatment_model,
            'control_model': control_model,
            'uplift_predictor': predict_uplift
        }
```

## ğŸ“‹ ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ

### ğŸ“Š åˆ†æãƒ¬ãƒãƒ¼ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
```python
class AnalysisReporter:
    def __init__(self):
        pass
    
    def generate_ab_test_report(self, test_results, business_impact):
        """
        A/Bãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        """
        report = {
            'executive_summary': {
                'test_name': test_results['test_name'],
                'duration': test_results['duration'],
                'sample_size': test_results['sample_size'],
                'primary_metric': test_results['primary_metric'],
                'result': 'Significant' if test_results['is_significant'] else 'Not Significant',
                'recommendation': self.generate_recommendation(test_results, business_impact)
            },
            
            'statistical_results': {
                'p_value': test_results['p_value'],
                'confidence_interval': test_results['confidence_interval'],
                'effect_size': test_results['effect_size'],
                'statistical_power': test_results['statistical_power']
            },
            
            'business_impact': {
                'revenue_impact': business_impact['revenue_impact'],
                'user_impact': business_impact['user_impact'],
                'roi': business_impact['roi'],
                'implementation_cost': business_impact['implementation_cost']
            },
            
            'risks_and_considerations': {
                'technical_risks': business_impact['technical_risks'],
                'user_experience_impact': business_impact['ux_impact'],
                'long_term_considerations': business_impact['long_term_considerations']
            }
        }
        
        return report
    
    def generate_recommendation(self, test_results, business_impact):
        """
        æ¨å¥¨äº‹é …ã®ç”Ÿæˆ
        """
        if test_results['is_significant'] and business_impact['roi'] > 150:
            return "Implement: çµ±è¨ˆçš„ã«æœ‰æ„ã§ã€ROIãŒååˆ†ã«é«˜ã„"
        elif test_results['is_significant'] and business_impact['roi'] > 0:
            return "Consider: çµ±è¨ˆçš„ã«æœ‰æ„ã ãŒã€ROIã‚’æ…é‡ã«æ¤œè¨"
        elif not test_results['is_significant']:
            return "Do not implement: çµ±è¨ˆçš„ã«æœ‰æ„ã§ãªã„"
        else:
            return "Further analysis needed: è¿½åŠ åˆ†æãŒå¿…è¦"
```

## ğŸ”§ ãƒ„ãƒ¼ãƒ«ãƒ»ãƒªã‚½ãƒ¼ã‚¹

### åˆ†æãƒ„ãƒ¼ãƒ«
- [Python](https://www.python.org/) - ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°
- [R](https://www.r-project.org/) - çµ±è¨ˆè§£æ
- [Jupyter Notebook](https://jupyter.org/) - åˆ†æç’°å¢ƒ
- [Tableau](https://www.tableau.com/) - ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–

### çµ±è¨ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒª
- [SciPy](https://scipy.org/) - çµ±è¨ˆé–¢æ•°
- [scikit-learn](https://scikit-learn.org/) - æ©Ÿæ¢°å­¦ç¿’
- [statsmodels](https://www.statsmodels.org/) - çµ±è¨ˆãƒ¢ãƒ‡ãƒ«
- [pandas](https://pandas.pydata.org/) - ãƒ‡ãƒ¼ã‚¿å‡¦ç†

### å¯è¦–åŒ–ãƒ„ãƒ¼ãƒ«
- [Matplotlib](https://matplotlib.org/) - åŸºæœ¬çš„ãªå¯è¦–åŒ–
- [Seaborn](https://seaborn.pydata.org/) - çµ±è¨ˆå¯è¦–åŒ–
- [Plotly](https://plotly.com/) - ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–å¯è¦–åŒ–
- [Grafana](https://grafana.com/) - ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

### é€£çµ¡å…ˆ
- ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ãƒãƒ¼ãƒ ãƒªãƒ¼ãƒ€ãƒ¼: ds-lead@your-company.com
- çµ±è¨ˆå°‚é–€å®¶: stats@your-company.com
- ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼: pm@your-company.com

## ğŸ“š å­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹

### ğŸ“ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
- [çµ±è¨ˆå­¦åŸºç¤](./training/statistics-fundamentals.md)
- [A/Bãƒ†ã‚¹ãƒˆåˆ†æ](./training/ab-test-analysis.md)
- [æ©Ÿæ¢°å­¦ç¿’å¿œç”¨](./training/machine-learning-applications.md)

### ğŸ“– å‚è€ƒè³‡æ–™
- [çµ±è¨ˆæ‰‹æ³•ã‚¬ã‚¤ãƒ‰](./guides/statistical-methods.md)
- [A/Bãƒ†ã‚¹ãƒˆãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹](./guides/ab-test-best-practices.md)
- [å› æœæ¨è«–å…¥é–€](./guides/causal-inference-guide.md)

---

## ğŸ¯ åˆ†æã®æˆåŠŸæŒ‡æ¨™

### ğŸ“Š åˆ†æå“è³ªæŒ‡æ¨™
- **çµ±è¨ˆçš„æ¤œå‡ºåŠ›**: 80%ä»¥ä¸Š
- **åŠ¹æœã‚µã‚¤ã‚ºæ¤œå‡º**: å®Ÿç”¨çš„ãªåŠ¹æœã‚µã‚¤ã‚ºã®æ¤œå‡º
- **åˆ†æç²¾åº¦**: äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®AUC 0.7ä»¥ä¸Š
- **ãƒ¬ãƒãƒ¼ãƒˆå“è³ª**: ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼æº€è¶³åº¦90%ä»¥ä¸Š

### ğŸ“ˆ ç¶™ç¶šçš„æ”¹å–„
- é€±æ¬¡ã§ã®åˆ†æçµæœãƒ¬ãƒ“ãƒ¥ãƒ¼
- æœˆæ¬¡ã§ã®æ‰‹æ³•æ”¹å–„
- å››åŠæœŸã§ã®åˆ†ææˆ¦ç•¥è¦‹ç›´ã—

**æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**: [A/Bãƒ†ã‚¹ãƒˆè¨­è¨ˆ](./ab-test-design.md)ã‹ã‚‰å§‹ã‚ã¾ã—ã‚‡ã†ï¼